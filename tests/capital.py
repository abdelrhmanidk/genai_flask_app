import os
from dotenv import load_dotenv
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames

# Load environment variables
load_dotenv()

credentials = Credentials(
                   url = os.getenv("WATSONX_URL"),
                   api_key = os.getenv("WATSONX_API_KEY")
                  )



params = {
    GenTextParamsMetaNames.DECODING_METHOD: "greedy",
    GenTextParamsMetaNames.MAX_NEW_TOKENS: 100
}



model = ModelInference(
    model_id='meta-llama/llama-3-2-11b-vision-instruct',
    params=params,
    credentials=credentials,
    project_id=os.getenv("WATSONX_PROJECT_ID")
)


text = """
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are an expert assistant who provides concise and accurate answers.<|eot_id|>

<|start_header_id|>user<|end_header_id|>
What is the capital of Canada?<|eot_id|>

<|start_header_id|>assistant<|end_header_id|>
"""

print(model.generate(text)['results'][0]['generated_text'])


